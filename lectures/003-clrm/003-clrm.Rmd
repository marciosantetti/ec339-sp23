---
title: ".b[The Classical Linear Regression Model (CLRM)]"
subtitle: ".b[.green[EC 339]]"
author: "Marcio Santetti"
date: "Spring 2023"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'skid-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel, xaringanExtra, tidyverse, sjPlot, mathjaxr, ggforce, furrr, kableExtra, wooldridge, hrbrthemes, scales, ggeasy, patchwork, MetBrewer, stargazer, gapminder)




# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F,
  dpi=300
)

theme_set(theme_ipsum_rc())
```



# Motivation




---

# OLS works, but it needs assumptions

<br><br>

- The goal when using OLS is to obtain .b[unbiased], .b[efficient], and .b[consistent] estimators.

--

- Moreover, we want to be able to do .b[hypothesis testing].

--

- All these properties are made possible through .b[7 assumptions].

--

- This set of assumptions is known as the .b[Classical Linear Regression Model] (CLRM).


---
class: inverse, middle

# The Classical Assumptions


---

# The set of Classical Assumptions

<br>

**1**. The regression model is .b[linear], .b[correctly specified], and has an .b[additive] stochastic error
term.

--

**2**. The stochastic error term $(u_i)$ has a .b[zero] population mean.

--

**3**. All explanatory variables $(x_i)$ are .b[uncorrelated] with the error term.

--

**4**. Observations of the error term are .b[uncorrelated] with each other.

--

**5**. The error term has a .b[constant variance].

--

**6**. No explanatory variable is a .b[perfect linear function] of any other explanatory variable.

--

**7**. The error term is .b[normally distributed].

---

# Assumption 1

> "*The regression model is .b[linear], .b[correctly specified], and has an .b[additive] stochastic error
term.*"

--

  - .it[Linear] means linear in .b[parameters] $(\beta_i)$;
  
  - .it[Correctly specified] means that it has the correct .b[functional form] and .b[no] omitted variables.
  
  - And an .b[additive] error term implies .b[no] other form in which $u_i$ appears in a model.
  
--

<br>

- **Examples**:

$$
\begin{align}
y_i = \beta_0 \beta_1x_{1i} + \beta_2x_{2i} + u_i
\end{align}
$$
$$
\begin{align}
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i}u_i
\end{align}
$$
$$
\begin{align}
y_i = \beta_0 + log(\beta_1)x_{1i} + \beta_2x_{2i} + u_i
\end{align}
$$

---

# Assumption 1

One of the main reasons for a .it[violation] of CLRM Assumption I is an .b[incorrectly specified] model.

--

- This may happen due to

  - Incorrect .b[functional form] (data visualization matters!);

  - .b[Omitted] variables (leading to omitted variables bias).
  
--


<br>


A regression's error term may sometimes be a .b[black box].

--

  - Recall that any potentially omitted variable(s) lie(s) there!

--

Therefore, our models must have a .b[theoretical] motivation.


---

# What is bias?


An estimator is .b[biased] if its expected value is different from the *true* population parameter.

--

When considering our slope coefficients $(\hat{\beta}_i)$, we expect that they, on average, are close to the .b["true"] population parameter, $\beta_{pop}$.

.pull-left[

**Unbiased:** $\mathop{\mathbb{E}}\left[ \hat{\beta}_{OLS} \right] = \beta_{pop}$

```{r, echo = F, dev = "svg", fig.height=6}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "#0072b2", alpha = 0.7) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

--

.pull-right[

**Biased:** $\mathop{\mathbb{E}}\left[ \hat{\beta}_{OLS} \right] \neq \beta_{pop}$

```{r, echo = F, dev = "svg", fig.height=6}

tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "#d55e00", alpha = 0.7) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]


---

# Assumption 2

> *"The stochastic error term $(u_i)$ has a .b[zero] population mean."*

--

<br>

  - Values of the stochastic error term are defined by .b[pure chance].
  
  - It follows a probability .b[distribution] centered around zero.
  
  - Also known as the .b[exogeneity] assumption.
  
--

<br>

From standard Microeconomic theory, recall:

  - Factors that influence the .b[demand] for a given good:
  
    - Price of the good itself, price of substitutes, preferences...
  
---

# Assumption 2

<br>

> *"The stochastic error term $(u_i)$ has a .b[zero] population mean."*


<br><br>


In practice, what is the difference between $\mathbb{E}[u \ | \ x] = 0$ and $\mathbb{E}[u \ | \ x] \neq 0$?

---

# Assumption 3

> *"All explanatory variables $(x_i)$ are .b[uncorrelated] with the error term."*

--

<br><br>

  - Observed values of the independent variable are determined .b[independently] of the values contained in the error term
  
  - $Cor(x_i, u_i) \neq 0 \implies$ .b[violation] of CLRM Assumption III.
  
  - A possible reason: a variable correlated with some $x_i$ being .b[omitted] from the model.


---

# Assumption 4

<br><br>

> *"Observations of the error term are .b[uncorrelated] with each other."*

--

<br>

  - Also known as .b[autocorrelation].
  
  - Common in .b[time-series] data.
  
  - Occurs when the model's disturbances are correlated .b[over time], i.e., $Cor(u_t, u_j) \neq 0$ for $t \neq j$.



---

# Assumption 4

Behavior of $u_t$ over time (positive serial correlation)

```{r, positive auto u, echo = F, fig.height = 5, dev = "svg"}
# Number of observations
T <- 1e2
# Rho
rho <- 0.95
# Set seed and starting point
set.seed(1234)
start <- rnorm(1)
# Generate the data
ar_df <- tibble(
  t = 1:T,
  x = runif(T, min = 0, max = 1),
  e = rnorm(T, mean = 0, sd = 2),
  u = NA
)
for (x in 1:T) {
  if (x == 1) {
    ar_df$u[x] <- rho * start + ar_df$e[x]
  } else {
    ar_df$u[x] <- rho * ar_df$u[x-1] + ar_df$e[x]
  }
}
ar_df %<>% mutate(y = 1 + 3 * x + u)
# Plot disturbances over time
ggplot(data = ar_df,
  aes(t, u)
) +
geom_line(color = "black", size = 0.9, alpha = 0.4) +
geom_point(color = "black", size = 3) +
ylab("Residuals") +
xlab("Time") +
  easy_y_axis_title_size(13) +
  easy_x_axis_title_size(13)
```

---

# Assumption 4

Behavior of $u_t$ over time (negative serial correlation)

```{r, negative auto u, echo = F, fig.height = 5, dev='svg'}
# Number of observations
T <- 1e2
# Rho
rho <- -0.95
# Set seed and starting point
set.seed(1234)
start <- rnorm(1)
# Generate the data
ar_df <- tibble(
  t = 1:T,
  x = runif(T, min = 0, max = 1),
  e = rnorm(T, mean = 0, sd = 2),
  u = NA
)
for (x in 1:T) {
  if (x == 1) {
    ar_df$u[x] <- rho * start + ar_df$e[x]
  } else {
    ar_df$u[x] <- rho * ar_df$u[x-1] + ar_df$e[x]
  }
}
ar_df %<>% mutate(y = 1 + 3 * x + u)
# Plot disturbances over time
ggplot(data = ar_df,
  aes(t, u)
) +
geom_line(color = "black", size = 0.9, alpha = 0.4) +
geom_point(color = "black", size = 3) +
ylab("Residuals") +
xlab("Time") +
  easy_y_axis_title_size(13) +
  easy_x_axis_title_size(13)
```

---


# Assumption 5

> *"The error term has a .b[constant variance]."*

--

<br>

  - Also known as the .b[homoskedasticity] assumption.
  
  - If violated, we have .b[heteroskedasticity].
  
  - Extremely .b[common] in cross-section data sets (also in financial time-series data).
  
--

<br>

- This assumption implies that the error term has the .b[same variance] for each value of the independent variable.

  - $Var(u|x) = \sigma^2$
  

---

# Assumption 5

- .b[Homoskedastic] residuals:

```{r, dev = "svg", echo = F, fig.height = 5}
library(patchwork)
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.6) +
labs(x = "Independent variable (x)", y = "Error term (u)") + geom_hline(yintercept = 0, color = "#d55e00",
                                                                        size=2) +
  easy_y_axis_title_size(13) +
  easy_x_axis_title_size(13)

```



---

# Assumption 5

- .b[Heteroskedastic] residuals:

```{r, dev = "svg", echo = F, fig.height = 5, message=FALSE, warning=FALSE}
set.seed(12345)
plt1 <- ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "Independent variable (x)", y = "Error term (u)") + geom_hline(yintercept = 0, color = "#d55e00",
                                                                        size=2)  +
  easy_y_axis_title_size(13) +
  easy_x_axis_title_size(13)
```


```{r, dev = "svg", echo = F, fig.height = 5}
set.seed(12345)
plt2 <- ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "Independent variable (x)", y = "Error term (u)") + geom_hline(yintercept = 0, color = "#d55e00",
                                                                        size=2) +
  easy_y_axis_title_size(13) +
  easy_x_axis_title_size(13)

plt1 | plt2
```


---

# Assumption 6

> *"No explanatory variable is a .b[perfect linear function] of any other explanatory variable."*

--

<br><br>

  - Also known as the .b[no perfect multicollinearity] assumption.
  
  - Only completely .b[violated] if an independent variable $x_i$ is a .b[deterministic] function of another variable $x_j$, for $i \neq j$
  
--

<br>

Examples:

  - $x_3 = x_1 - 1,000$

  - $x_2 = 50 + x_1$
  

---


# Assumption 7

<br><br>


> *"The error term is .b[normally distributed]."*

--

<br><br>

  - Summarized by $u_i \sim{\mathcal{N}(0,  \sigma^2)}$.
  
--

<br>

OLS .b[still works] without this assumption!

--

But crucial for .b[hypothesis testing and inference].

---


layout: false
class: inverse, middle

# The Gauss-Markov theorem


---

# The Gauss-Markov theorem

<br><br>

From CLRM Assumptions .b[I through VI], we guarantee that OLS is .hi-blue[BLUE].

--

<br><br>

We will learn how to deal with the most common .b[violations] of CLRM Assumption after the Midterm exam.







---

layout: false
class: inverse, middle

# Next time: CLRM in practice


---
exclude: true