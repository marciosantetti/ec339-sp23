---
title: ".b[Simple Linear Regression]"
subtitle: ".b[.green[EC 339]]"
author: "Marcio Santetti"
date: "Fall 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'skid-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel, xaringanExtra, tidyverse, sjPlot, showtext, mathjaxr, ggforce, furrr, kableExtra, wooldridge, hrbrthemes, scales, ggeasy, patchwork, MetBrewer)




# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F,
  dpi=300
)

theme_set(theme_ipsum_rc())
```

```{R, colors, include = F}
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#E02C05"
blue <- "#2b59c3"
green <- "#0FDA6D"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
```


# Motivation

---

# On notation

In our course, we will adopt the following .hi[notation] for a regression model:

<br>

$$ 
\begin{align}
y_i = \beta_0 + \beta_1x_{1i} + u_i 
\end{align}
$$
--

<br>

- where:

 - $y_i$: .hi[dependent variable]'s value for the $i^{th}$ individual;
 - $x_i$: .hi-orange[independent variable]'s value for the $i^{th}$ individual;
 - $\beta_0$: .hi[intercept] term;
 - $\beta_1$: .hi-orange[slope] coefficient;
 - $u_i$: .hi[residual/error] term (the $i^{th}$ individual's .hi-orange[random] deviation from the population parameters).

---


layout: false
class: inverse, middle

# Motivating regression models


---





# Data are fuzzy

.small[Life expectancy _vs._ GDP per capita (1952&mdash;2007):<sup>*<sup>]

```{r, echo=F, message=F, warning=F, dev = "svg", fig.height = 5, warning = FALSE}

library(gapminder)
library(ggplot2)
library(tidyverse)


data(gapminder)

p <- ggplot(gapminder, aes(log(gdpPercap), lifeExp))
p + geom_point(aes(color = continent), alpha=0.5, size = 1.3) +
  MetBrewer::scale_color_met_d("Archambault") +
  labs(x="GDP per capita (log)", y="Life Expectancy") +
  easy_y_axis_title_size(13) +
  easy_x_axis_title_size(13) +
  easy_plot_legend_size(13) 
  



```


.pull-left[.footnote[
[*]: Data from [`Gapminder`](https://www.gapminder.org).
]]

---


# Data are fuzzy

.small[Now, including .hi[regression lines]:]

```{r, echo=F, message=F, warning=F, dev = "svg", fig.height = 5}



p + geom_point(aes(color = continent), alpha=0.5, size = 1.3) +
  MetBrewer::scale_color_met_d("Archambault") +
  geom_smooth(aes(color=continent), method="lm", se=FALSE) +
  labs(x="GDP per capita (log)", y="Life Expectancy") +
  easy_y_axis_title_size(13) +
  easy_x_axis_title_size(13) +
  easy_plot_legend_size(13) 



```


---

# Data are fuzzy

.small[Narrowing down to the Americas:]

```{r, echo=F, message=F, warning=F, dev = "svg", fig.height = 5}


gapminder %>% filter(continent == 'Americas') %>% 
  ggplot(aes(x=log(gdpPercap), y=lifeExp)) +
  geom_point(aes(color = country), alpha=0.5, size = 1.3) +
  #MetBrewer::scale_color_met_c("VanGogh1", n = 30) +
  geom_smooth(aes(color=country), method="lm", se=FALSE) +
  labs(x="GDP per capita (log)", y="Life Expectancy") +
  easy_y_axis_title_size(13) +
  easy_x_axis_title_size(13) +
  easy_plot_legend_size(13) 


```


---

# Data are fuzzy

Now, for the US...

```{r, echo=F, message=F, warning=F, dev = "svg", fig.height = 5}


gapminder %>% filter(country == 'United States') %>% 
  ggplot(aes(x=gdpPercap, y=lifeExp)) + scale_x_log10() + geom_point(color="brown1", alpha=0.7, size = 2.3) + 
  geom_smooth(method="lm", se=F, color="darkolivegreen4") +
  labs(x="GDP per capita", y="Life Expectancy") +
  easy_x_axis_title_size(13) +
  easy_y_axis_title_size(13)


```


---



layout: false
class: inverse, middle

# Which method to use?


---

# Ordinary Least Squares (OLS) 

<br>

The .hi[Ordinary Least Squares (OLS) Estimator]:

<br>

  - OLS .hi-orange[minimizes] the .it[squared distance] between the data points and the regression line it generates.

  - This way, we are .hi[minimizing] _error_ (_ignorance_) about our data and the relationship we are trying to better understand.

  - In addition, it is .hi-orange[easy] to estimate and interpret.


---

# Ordinary Least Squares (OLS) 

The .hi[Ordinary Least Squares (OLS) Estimator]:


.center[

$\text{SSR} = \sum_{i = 1}^{n} u_i^2\quad$ where $\quad u_i = y_i - \hat{y}_i$

]

--

<br>

- Why .hi-orange[squaring] these residuals? 

--

- Bigger errors, bigger .hi[penalties].

--

$$ 
\begin{align}
\min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SSR} \\
\min_{\hat{\beta}_0,\, \hat{\beta}_1} (y_i - \hat{y}_i)^2 \\
\min_{\hat{\beta}_0,\, \hat{\beta}_1} \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2
\end{align}
$$
---

# Ordinary Least Squares (OLS) 

The .hi[Ordinary Least Squares (OLS) Estimator]:

<br>

- .hi[Slope coefficient]:

$$ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} = \dfrac{Cov(x,y)}{Var(x)} $$


--

- .hi-orange[Intercept coefficient]:

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

---

# "Best" regression lines


```{R, gen dataset, include = F, cache=F}
# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(321)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 10, sd = 1.2),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.6 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)

# Regressions
lm0 <- lm(y ~ x, data = pop_df)
```


```{R, ols vs lines 1, echo = F, dev = "svg", fig.height = 5}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 3, color = "darkslategray", alpha = 0.8) +
  easy_x_axis_title_size(13) +
  easy_y_axis_title_size(13)
```



---

# "Best" regression lines

For any line &mdash; $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$


```{R, vs lines 2, echo = F, dev = "svg", fig.height = 5}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 3, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "turquoise", size = 2, alpha = 0.9) +
  easy_x_axis_title_size(13) +
  easy_y_axis_title_size(13)
```

---

# "Best" regression lines

For any line &mdash; $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ &mdash;, we can calculate residuals: $u_i = y_i - \hat{y}_i$

```{R, ols vs lines 3, echo = F, dev = "svg", fig.height = 5}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 3, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "turquoise", size = 2, alpha = 0.9) +
  easy_x_axis_title_size(13) +
  easy_y_axis_title_size(13)
```

---

# "Best" regression lines

For any line &mdash; $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ &mdash;, we can calculate residuals: $u_i = y_i - \hat{y}_i$

```{R, ols vs lines 4, echo = F, dev = "svg", fig.height = 5}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 5
b1 <- 0.5
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 3, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "turquoise", size = 2, alpha = 0.9) +
  easy_x_axis_title_size(13) +
  easy_y_axis_title_size(13)
```

---

# "Best" regression lines

For any line &mdash; $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ &mdash;, we can calculate residuals: $u_i = y_i - \hat{y}_i$

```{R, ols vs lines 5, echo = F, dev = "svg", fig.height = 5}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 14
b1 <- -0.4
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 3, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "turquoise", size = 2, alpha = 0.9) +
  easy_x_axis_title_size(13) +
  easy_y_axis_title_size(13)
```


---

# "Best" regression lines


SSR squares the errors $\left(\sum u_i^2\right)$: bigger errors get bigger penalties.

```{R, ols vs lines 6, echo = F, dev = "svg", fig.height = 5}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 14
b1 <- -0.4
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 3, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "turquoise", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
  easy_add_legend_title("Squared errors") 
```

---

# "Best" regression lines


The OLS estimate is the combination of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize SSR.

```{R, ols vs lines 7, echo = F, dev = "svg", fig.height = 5}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 3, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
  easy_add_legend_title("Squared errors")
```


---

layout: false
class: inverse, middle

# Interpretation

---



# Interpreting OLS coefficients

<br>

- .hi-orange[Slope] coefficient: the change (increase/decrease) in the dependent variable $(y)$ generated by a 1-unit increase in the independent variable $(x)$.


- .hi[Intercept] term: the value of the dependent variable $(y)$ when $x=0$.

--


<br>

.hi[Example]:

- Interpret the following estimated regression models:


$$
\begin{align}
\widehat{wage_i} = 10 + 2.65 \ educ_i
\end{align}
$$

--

$$
\begin{align}
\widehat{sleep_i} = 6.5 -0.65 \ kids_i
\end{align}
$$

---

layout: false
class: inverse, middle

# Next time: Simple regression in practice


---
exclude: true


