---
title: ".b[Omitted Variables Bias (OVB)]"
subtitle: ".b[.green[EC 339]]"
author: "Marcio Santetti"
date: "Fall 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'skid-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel, xaringanExtra, tidyverse, sjPlot, mathjaxr, ggforce, furrr, kableExtra, wooldridge, hrbrthemes, scales, ggeasy, patchwork, MetBrewer, stargazer, gapminder, lmtest)




# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F,
  dpi=300
)

theme_set(theme_ipsum_rc())
```

```{R, colors, include = F}
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#E02C05"
blue <- "#2b59c3"
green <- "#0FDA6D"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
met_slate <- "#23373b" 
```



# Motivation

---

# Well-specified models

Recall .hi[CLRM Assumption I]: 

  > "*The regression model is .green[linear], .green[correctly specified], and has an .green[additive] stochastic error
term*."

--

<br>

The .green[hardest] part regarding this assumption is to have a .hi[well-specified model].

--

Suppose we have the following model:

$$
\begin{align}
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \beta_3x_{3i} + u_i
\end{align}
$$
--

<br>

  - How can we .green[evaluate] whether this is a well-specified model?
  - Does it have the appropriate .green[functional form]?
  - Is this model in accordance with .green[economic theory]?
  
  
---

# Well-specified models

In fact, we can .green[never know for sure] if we have the most appropriate model.

--

.hi[Theory] is always (and will always be) the best guide.

--

<br>

In addition, we must always .hi[visualize] our data, knowing it better in order to define the model's .green[functional form].

--

<br>

  -  __A different functional form may also be an omitted variable!__

  - For instance, if the .green['true'] model contains a squared term, in case we omit it from our sample regression model, it will be .hi[misspecified].


---

layout: false
class: inverse, middle

# The nature of the problem


---

# Recalling bias

An estimator is .hi[biased] if its expected value is different from the *true* population parameter.

--

When considering our slope coefficients $(\hat{\beta}_i)$, we expect that they, on average, are close to the .green["true"] population parameter, $\beta_{pop}$.

--

.pull-left[

**Unbiased:** $\mathop{\mathbb{E}}\left[ \hat{\beta}_{OLS} \right] = \beta_{pop}$

```{R, unbiased pdf, echo = F, dev = "svg", fig.height=5.5}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = turquoise, alpha = 0.4) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

--

.pull-right[

**Biased:** $\mathop{\mathbb{E}}\left[ \hat{\beta}_{OLS} \right] \neq \beta_{pop}$

```{R, biased pdf, echo = F, dev = "svg", fig.height=5.5}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = met_slate, alpha = 0.4) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

---


# Omitting a relevant variable

- Assume we know the .hi[true] population model:

$$
\begin{align}
y_i^{true} = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + u_i   
\end{align}
$$


--

<br>

- And we estimate the following model:


$$
\begin{align}
y_i = \beta_0 + \beta_1x_{1i} + u_i^*  
\end{align}
$$

with 

$$
\begin{align}
u_i^*  = u_i + \beta_2x_{2i} 
\end{align}
$$

--

- Assuming that $x_1$ and $x_2$ (the omitted variable) share some degree of .green[correlation] (which is usually the case), the error term is no longer .hi[independent] of an explanatory variable, as per .green[CLRM Assumption III]. 


---


# Omitting a relevant variable

<br>

- Consider a simple .green[demand model]:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

data <- read_csv('newbroiler.csv')

```


$$
\small
\begin{align}
log(qchicken_i) = {} & \beta_0 + \beta_1pchicken_{i} + \beta_2pbeef_{i}  + \beta_3dispinc_{i} + \beta_4log(xchicken_i) + u_i
\end{align}
$$
--

<br>

- And we .green[estimate] it:

$$
\small
\begin{align}
\widehat{log(qchicken_i)} = {} & 2.95 - 0.23 \ pchicken_{i} +  0.18 \ pbeef_{i} + \\ & + 0.000036 \ dispinc_{i} + 
0.75 \ log(xchicken_i) 
\end{align}
$$

---

# Omitting a relevant variable

- And now we omit `dispinc` from the model:

$$
\small
\begin{align}
\widehat{log(qchicken_i)} =  3.49 - 0.30 \ pchicken_{i} +  0.25 \ pbeef_{i}  + 
1.65 \ log(xchicken_i) 
\end{align}
$$
--

<br>

- This model's .green[residual] term contains `dispinc`. 

<br>

--

- Let us check out the .green[correlation coefficient] between `dispinc` and other variables:

<br>

```{r, echo=FALSE}
data %>% summarize(corr_y_pchicken = cor(y, p),
                   corr_y_pbeef = cor(y,pb),
                   corr_y_x = cor(y, lexpts)) %>% kable()            
```

---

# Omitting a relevant variable

<br>

```{r, echo=FALSE, message=FALSE}

model_true <- lm(log(q) ~ p + pb + lexpts + y, data = data)
model_omitted <- lm(log(q) ~ p + pb + lexpts, data = data)
model_irrelevant <- lm(log(q) ~ p + pb + lexpts + y + popgro, data = data) 

```

```{r, echo=FALSE}

model_true %>% tidy() %>% 
  kable(caption = "'True' model")

```

<br>


```{r, echo=FALSE}
model_omitted %>% tidy() %>% 
  kable(caption = "Biased model")
```

---

layout: false
class: inverse, middle

# Including irrelevant variables

---

# Including irrelevant variables

<br>

- Now assume that the .hi[true] model is:


$$
\begin{align}
y_i^{true} = \beta_0 + \beta_1x_{1i} + u_i   
\end{align}
$$


--

<br>

- And, instead, we estimate

$$
\begin{align}
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + u_i^*   
\end{align}
$$

with 

$$
\begin{align}
u_i^*  = u_i - \beta_2x_{2i} 
\end{align}
$$

---


# Including irrelevant variables

<br>

- Suppose we add `popgro`, a variable measuring .green[population growth], to our original model:


<br>


$$
\begin{align}
\small
\widehat{log(qchicken_i)} = {} & 2.89 - 0.23 \ pchicken_{i} +  0.19 \ pbeef_{i} + \\ & + 0.000038 \ dispinc_{i} + 
0.69 \ log(xchicken_i) + \\ & + 0.017 \ popgro_t
\end{align}
$$

---

# Including irrelevant variables



```{r, echo=FALSE}

model_true %>% tidy() %>% 
  kable(caption = "'True' model")

```

<br>


```{r, echo=FALSE}
model_irrelevant %>% tidy() %>% 
  kable(caption = "Model with irrelevant variable")
```

---

layout: false
class: inverse, middle

# The .mono[RESET] test


---

# The .mono[RESET] test

<br>

Knowing for sure whether our models suffer from Omitted Variables Bias (OVB) is .green[hard].

--

However, the .green[.mono[RESET] test for functional form misspecification] can help us.

--

<br>

It consists of running an .hi[F-test] on .hi[functional forms] of the .hi[fitted values] of the dependent variable $(\hat{y})$.

--

These functional forms $(\hat{y}^2, \hat{y}^3, etc.)$ serve as _.hi[proxies]_ for potentially omitted variables.

--

<br>

Recall that .green[functional forms] of .green[already included independent variables] can also be omitted variables!

---

# The .mono[RESET] test

### The .hi[recipe] `r emo::ji("woman_cook")` `r emo::ji("man_cook")`:

<br>

.pseudocode-small[

1. Estimate the regression model via OLS;

2. Store the regression's fitted values $(\hat{y}_i)$;

3. Use functional forms of $\hat{y}_i$ (squared, cubic terms, etc.) as **independent variables** in a new model;

4. Compare the fits of models from step **1** and **3** through an *F-test*;

5. In case these additional terms are **not** jointly significant, we do not suspect of omitted variables.

6. In case these terms are *jointly significant*, we should consider adding new regressors to the original model.


]

---

# The .mono[RESET] test

.pseudocode-small[

Estimate the regression model via OLS

]

$$
\begin{align}
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + u_i   
\end{align}
$$
--

.pseudocode-small[

Store the regression's fitted values $(\hat{y}_i)$

]

$$
\begin{align}
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_{1i} + \hat{\beta}_2x_{2i}
\end{align}
$$

--

.pseudocode-small[

Use functional forms of $\hat{y}_i$ (squared, cubic terms, etc.) as **independent variables** in a new model

]

$$
\begin{align}
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \beta_3\hat{y}_i^2 + \beta_4\hat{y}_i^3  + u_i
\end{align}
$$

--

.pseudocode-small[

Compare the fits of models from step **1** and **3** through an *F-test*

]


- $H_0: \hat{\beta}_3 = \hat{\beta}_4  = 0$
- $H_a: H_0$ is not true


---

# The .mono[RESET] test

- In case the .hi[null hypothesis] is .hi[rejected], then we have evidence of omitted variables.

--

- In case we .hi[do not reject] $H_0$, then we can stick with the original model.


--


In .mono[R]...

```{r}
resettest(model_true, power = 2:4)

```

<br>


What do we conclude?



---

# The .mono[RESET] test

<br>

In .mono[Stata]...



```{}
estat ovtest

Ramsey RESET test for omitted variables
Omitted: Powers of fitted values of lq

H0: Model has no omitted variables

F(3, 43) =   1.64
Prob > F = 0.1953


```

<br>


What do we conclude?

---

layout: false
class: inverse, middle

# Next time: OVB in practice


---
exclude: true
